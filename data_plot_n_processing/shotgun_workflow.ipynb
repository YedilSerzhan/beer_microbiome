{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d8195d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioblend.galaxy import GalaxyInstance\n",
    "from bioblend.galaxy import dataset_collections\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3dce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'https://usegalaxy.eu/'\n",
    "api_key = os.environ['my_galaxy_api']\n",
    "gi = GalaxyInstance(server, key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fcff38c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_class': 'History',\n",
       "  'id': 'fbd2f0e6fce8bebc',\n",
       "  'name': 'PRJNA390460',\n",
       "  'deleted': False,\n",
       "  'purged': False,\n",
       "  'url': '/api/histories/fbd2f0e6fce8bebc',\n",
       "  'published': False,\n",
       "  'annotation': None,\n",
       "  'tags': [],\n",
       "  'update_time': '2023-02-01T09:52:03.754896'},\n",
       " {'model_class': 'History',\n",
       "  'id': 'ee9cf18ea49d2fb1',\n",
       "  'name': 'qiime2 test',\n",
       "  'deleted': False,\n",
       "  'purged': False,\n",
       "  'url': '/api/histories/ee9cf18ea49d2fb1',\n",
       "  'published': False,\n",
       "  'annotation': None,\n",
       "  'tags': [],\n",
       "  'update_time': '2023-01-30T19:11:02.169097'},\n",
       " {'model_class': 'History',\n",
       "  'id': 'ecfc8c06ceae22af',\n",
       "  'name': 'shotgun paired test run',\n",
       "  'deleted': False,\n",
       "  'purged': False,\n",
       "  'url': '/api/histories/ecfc8c06ceae22af',\n",
       "  'published': False,\n",
       "  'annotation': None,\n",
       "  'tags': [],\n",
       "  'update_time': '2023-01-24T17:43:51.334962'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all history\n",
    "# gi.histories.get_histories()\n",
    "\n",
    "# show specific history with ID\n",
    "# gi.histories.show_history('ecfc8c06ceae22af', contents=False)\n",
    "\n",
    "# show dataset with id\n",
    "# gi.datasets.show_dataset('4838ba20a6d86765e865eafb64b12ee6')\n",
    "\n",
    "# show all workflows\n",
    "# gi.workflows.get_workflows()\n",
    "\n",
    "# get specific workflow with id\n",
    "# wf_shotgun_paired = gi.workflows.show_workflow('dde25e07f2db3f7c')\n",
    "\n",
    "# show workflow inputs\n",
    "# wf_shotgun_paired['inputs']\n",
    "\n",
    "# upload file locally\n",
    "# gi.tools.upload_file('test.txt', 'f3c2b0f3ecac9f02')\n",
    "\n",
    "# upload file using FTP when it's over 2 G\n",
    "# gi.tools.upload_from_ftp('test.txt', 'f3c2b0f3ecac9f02')\n",
    "\n",
    "# upload data with urls\n",
    "# gi.tools.put_url(content=dataset_links, history_id=history_id)\n",
    "\n",
    "# create dataset collection\n",
    "# collection_response = gi.histories.create_dataset_collection(\n",
    "#     history_id='d786d448a802ae4b',\n",
    "#     collection_description=dataset_collections.CollectionDescription(\n",
    "#         name=\"MyListOfPairedDatasets\",\n",
    "#         type=\"list:paired\",\n",
    "#         elements=[\n",
    "#             dataset_collections.CollectionElement(\n",
    "#                 name=\"sample1\",\n",
    "#                 type=\"paired\",\n",
    "#                 elements=[\n",
    "#                     dataset_collections.HistoryDatasetElement(name=\"forward\", id='4838ba20a6d867655f39fcd3cfc9f5ca'),\n",
    "#                     dataset_collections.HistoryDatasetElement(name=\"reverse\", id='4838ba20a6d86765eeb232dc71866fdd'),\n",
    "#                 ]\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07c8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_to_history_id_info():\n",
    "    with open('projects.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def update_project_to_history_id_info(payload):\n",
    "    with open('projects.json','r+') as f:\n",
    "        file_data = json.load(f)\n",
    "        file_data.update(payload)\n",
    "        f.seek(0)\n",
    "        json.dump(file_data, f, indent = 4)\n",
    "\n",
    "def upload_dataset(gi, dataset_links, history_id):\n",
    "    r = gi.tools.put_url(\n",
    "        content=dataset_links,\n",
    "        history_id=history_id,\n",
    "    )\n",
    "    return r\n",
    "\n",
    "def create_paired_dataset_collection(dataset_list):\n",
    "    print(\"ha1\")\n",
    "    collection_elements = []\n",
    "    for i in range(0, len(dataset_list), 2):\n",
    "        ce = dataset_collections.CollectionElement(\n",
    "            name=(\"sample\"+str(len(collection_elements)+1)),\n",
    "            type=\"paired\",\n",
    "            elements=[\n",
    "                dataset_collections.HistoryDatasetElement(name=\"forward\", id=dataset_list[i]['id']),\n",
    "                dataset_collections.HistoryDatasetElement(name=\"reverse\", id=dataset_list[i+1]['id']),\n",
    "            ]\n",
    "        )\n",
    "        collection_elements.append(ce)\n",
    "    print(\"ha2\")\n",
    "    return dataset_collections.CollectionDescription(\n",
    "        name=\"MyListOfPairedDatasets\",\n",
    "        type=\"list:paired\",\n",
    "        elements=collection_elements\n",
    "    )\n",
    "\n",
    "\n",
    "def monitor_workflow_execution(gi, invocation_id, max_attempts=100, sleep_time=60):\n",
    "    \"\"\"\n",
    "    Monitors the status of a workflow execution in Galaxy\n",
    "\n",
    "    Parameters:\n",
    "    gi: Galaxy instance\n",
    "    invocation_id: ID of the workflow invocation\n",
    "    max_attempts: Maximum number of status checks before giving up (default: 100)\n",
    "    sleep_time: Time in seconds to wait between checks (default: 60)\n",
    "    \"\"\"\n",
    "    # Open the CSV file in append mode\n",
    "    with open(\"workflow_status.csv\", \"a\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"Timestamp\", \"Invocation ID\", \"Attempt\", \"Status\"])\n",
    "\n",
    "        for attempt in range(max_attempts):\n",
    "            invocation = gi.workflows.show_invocation(invocation_id)\n",
    "            state = invocation['state']\n",
    "            timestamp = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            if state == 'new':\n",
    "                writer.writerow([timestamp, invocation_id, attempt + 1, \"Queued\"])\n",
    "            elif state == 'scheduled':\n",
    "                writer.writerow([timestamp, invocation_id, attempt + 1, \"Running\"])\n",
    "            elif state == 'ok':\n",
    "                writer.writerow([timestamp, invocation_id, attempt + 1, \"Completed\"])\n",
    "                return invocation\n",
    "            elif state in ['error', 'failed']:\n",
    "                writer.writerow([timestamp, invocation_id, attempt + 1, \"Error\"])\n",
    "                return invocation\n",
    "            \n",
    "            # if state is not one of the expected values, continue checking\n",
    "            writer.writerow([timestamp, invocation_id, attempt + 1, state])\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        # if reached max_attempts without a conclusive state, return the last fetched invocation\n",
    "        writer.writerow([timestamp, invocation_id, attempt + 1, \"Max attempts reached\"])\n",
    "    return invocation\n",
    "\n",
    "\n",
    "def shotgun_main(gi, dataset, workflow_id):\n",
    "\n",
    "    for project_id, dataset_link_list in dataset.items():\n",
    "        \n",
    "        # create history\n",
    "        print(f'creating new history for {project_id}')\n",
    "        history_id = gi.histories.create_history(project_id)['id']\n",
    "        print(f'new history with id {history_id} created for {project_id}')\n",
    "\n",
    "        # keep record of project to history id\n",
    "        update_project_to_history_id_info({project_id: history_id})\n",
    "        \n",
    "        # upload dataset\n",
    "        print(f'uploading datasets to {project_id}')\n",
    "        dataset_links = '\\n'.join(dataset_link_list)\n",
    "        upload_result = upload_dataset(gi, dataset_links, history_id)['outputs']\n",
    "        print(upload_result)\n",
    "        \n",
    "        # wait for uploading complete\n",
    "        cnt = 1\n",
    "        while True:\n",
    "            state_ids = gi.histories.show_history(history_id, contents=False)['state_ids']\n",
    "            if len(state_ids['ok']) == len(dataset_link_list):\n",
    "                print(f'uploading datasets to {project_id} successful')\n",
    "                break\n",
    "            else:\n",
    "                print(f'waitting for the dataset to be available {cnt}')\n",
    "                cnt = cnt + 1\n",
    "                time.sleep(60)\n",
    "        \n",
    "        # create dataset collection\n",
    "        cd = create_paired_dataset_collection(upload_result)\n",
    "        collection_response = gi.histories.create_dataset_collection(\n",
    "            history_id=history_id,\n",
    "            collection_description=cd\n",
    "        )\n",
    "        print(collection_response)\n",
    "        \n",
    "        # get the workflow, the dataset, create dataset map for workflow inputs\n",
    "        wf = gi.workflows.show_workflow(workflow_id)\n",
    "        dataset_id = collection_response['id']\n",
    "        dataset_map = {'src': 'hdca', 'id': dataset_id}\n",
    "        \n",
    "        # invoke workflow\n",
    "        invoke_response = gi.workflows.invoke_workflow(\n",
    "            wf['id'],\n",
    "            inputs={ wf['inputs']['0']['uuid']: dataset},\n",
    "            history_id=history_id,\n",
    "            inputs_by='step_uuid',\n",
    "        )\n",
    "\n",
    "        print(invoke_response)\n",
    "\n",
    "        # monitor workflow execution\n",
    "        invocation_id = invoke_response['id']\n",
    "        monitor_workflow_execution(gi, invocation_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76aa5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'PRJNA390460': ['ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR589/003/SRR5890763/SRR5890763_1.fastq.gz', 'ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR589/003/SRR5890763/SRR5890763_2.fastq.gz']}\n",
    "\n",
    "shotgun_main(gi, dataset, 'dde25e07f2db3f7c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac712d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chao1 paired t-test:\n",
      " t = 3.844832916010616\n",
      " p = 0.01206427941392628\n",
      "\n",
      "Shannon paired t-test:\n",
      " t = -4.536992108267604\n",
      " p = 0.006185714676929359\n",
      "\n",
      "Simpson paired t-test:\n",
      " t = 1.2573751347659914\n",
      " p = 0.2641501551547413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Create a dictionary representing your data\n",
    "data = {\n",
    "    'Sample': ['Extra #21', 'Extra #22', 'Extra #25', 'Rubi #11', 'Rubi #13', 'Rubi #9'],\n",
    "    'Chao1_Original': [48, 67.5, 98.6, 70.5, 90.2, 117.2],\n",
    "    'Chao1_Re-analysis': [42, 61, 94, 65, 89, 104],\n",
    "    'Shannon_Original': [2.07, 2.45, 2.53, 1.88, 2.16, 3.14],\n",
    "    'Shannon_Re-analysis': [2.50, 2.94, 3.31, 3.46, 2.71, 4.16],\n",
    "    'Simpson_Original': [0.83, 0.87, 0.86, 0.72, 0.77, 0.92],\n",
    "    'Simpson_Re-analysis': [0.76, 0.80, 0.82, 0.82, 0.65, 0.89]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "indexes = ['Chao1', 'Shannon', 'Simpson']\n",
    "\n",
    "for index in indexes:\n",
    "    t, p = stats.ttest_rel(df[f'{index}_Original'], df[f'{index}_Re-analysis'])\n",
    "    print(f\"{index} paired t-test:\\n t = {t}\\n p = {p}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
